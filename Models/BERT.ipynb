{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/waihengsoh/Desktop/Desktop – Wai Heng/Simply Solution/vehicle_dis/Equities_Price_Prediction_XAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 09:14:06.120774: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.Historical_News import historical_news\n",
    "from datetime import date, timedelta\n",
    "news = historical_news.get_all_news(date(2021, 8, 1), date(2021, 10, 31))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('/Users/waihengsoh/Desktop/Desktop – Wai Heng/Simply Solution/vehicle_dis/Equities_Price_Prediction_XAI/csv_files/news/train.csv')\n",
    "test_df.to_csv('/Users/waihengsoh/Desktop/Desktop – Wai Heng/Simply Solution/vehicle_dis/Equities_Price_Prediction_XAI/csv_files/news/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47041 47041 5227 5227 3263 3263\n"
     ]
    }
   ],
   "source": [
    "# news = historical_news.get_all_news(date(2021, 8, 1), date(2021, 10, 31))\n",
    "train = pd.read_csv(\"../csv_files/news/train.csv\")\n",
    "train = train.loc[:, ['content', 'price_change']]\n",
    "X_train = train['content'].tolist()\n",
    "y_train = train['price_change'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "test = pd.read_csv(\"../csv_files/news/test.csv\")\n",
    "test = test.loc[:, ['content', 'price_change']]\n",
    "X_test = test['content'].tolist()\n",
    "y_test = test['price_change'].tolist()\n",
    "\n",
    "print(len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None,\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None,\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                           test, \n",
    "                                                                           'DATA_COLUMN', \n",
    "                                                                           'LABEL_COLUMN')\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train[:47000], train[47000:], 'content', 'price_change')\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 5346s 53s/step - loss: 0.6934 - accuracy: 0.5669 - val_loss: 0.7138 - val_accuracy: 0.5309\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 5400s 54s/step - loss: 0.6662 - accuracy: 0.6134 - val_loss: 0.6909 - val_accuracy: 0.5311\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 5366s 54s/step - loss: 0.6917 - accuracy: 0.5437 - val_loss: 0.6940 - val_accuracy: 0.5309\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 5313s 53s/step - loss: 0.6956 - accuracy: 0.5131 - val_loss: 0.7055 - val_accuracy: 0.5309\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 5330s 53s/step - loss: 0.6911 - accuracy: 0.5412 - val_loss: 0.6906 - val_accuracy: 0.5309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd26fd919d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=3e-07, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=5, steps_per_epoch=100, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../saved_models/model10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../saved_models/model2.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('../saved_models/model3', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../saved_models/model9/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('../saved_models/model9/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_test[:100])):\n",
    "    print(label[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Negative actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Negative actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Negative actual: Negative\n",
      "predicted: Positive actual: Negative\n",
      "predicted: Positive actual: Positive\n",
      "0.38\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(X_test[:100], padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "correct, wrong = 0, 0\n",
    "for i in range(len(X_test[:100])):\n",
    "  print(\"predicted:\", labels[label[i]], \"actual:\", labels[int(y_test[i])])\n",
    "  if labels[label[i]] == labels[int(y_test[i])]:\n",
    "    correct += 1\n",
    "  else:\n",
    "    wrong += 1\n",
    "print(correct / (correct + wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references from : https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31123794a318c38813355d29e032308f1989c1650d27ec22783ecd4e025c111f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
